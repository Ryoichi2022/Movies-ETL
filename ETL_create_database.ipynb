{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in c:\\users\\happi\\anaconda3\\envs\\pythondata\\lib\\site-packages (2.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "\n",
    "    movie = dict(movie)\n",
    "    \n",
    "    \n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Add the function that takes in three arguments;\n",
    "# Wikipedia data, Kaggle metadata, and MovieLens rating data (from Kaggle)\n",
    "\n",
    "def extract_transform_load():\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    kaggle_metadata_df = pd.DataFrame(pd.read_csv(kaggle_file, low_memory=False))\n",
    "    ratings_df = pd.DataFrame(pd.read_csv(ratings_file))\n",
    "\n",
    "    # Open and read the Wikipedia data JSON file.\n",
    "    with open(wiki_file, mode='r') as file:\n",
    "            wiki_file_raw = json.load(file)\n",
    "        \n",
    "    # Write a list comprehension to filter out TV shows.\n",
    "    wiki_movies = [movie for movie in wiki_file_raw\n",
    "                   if 'No. of episodes' not in movie]\n",
    "\n",
    "    # Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "    #  and call the clean_movie function on each movie.\n",
    "    wiki_movies_clean = [clean_movie(movie) for movie in wiki_movies]\n",
    "\n",
    "    # Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    wiki_movies_df = pd.DataFrame(wiki_movies_clean)\n",
    "    \n",
    "    # Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    try:\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "                \n",
    "    except:\n",
    "        print('IMDb_id error captured.')\n",
    "        \n",
    "    wiki_movies_df.dropna(subset=['imdb_id'], inplace=True)\n",
    "    wiki_movies_df.drop_duplicates(subset=['imdb_id'], inplace=True)\n",
    "    \n",
    "    # Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "    \n",
    "    # Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "        \n",
    "    # Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    # Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "   \n",
    "    # Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,|.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "    # Add the parse_dollars function.\n",
    "    def parse_dollars(s):\n",
    "    \n",
    "        # If s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "    \n",
    "        # Values $###.# million.\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.I):\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "            value = float(s) *10**6\n",
    "            return value\n",
    "                \n",
    "        # Values $###.# billion.\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.I):\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "            value = float(s) *10**9\n",
    "            return value\n",
    "                \n",
    "        # Values $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,|.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.I):\n",
    "            s = re.sub('\\$|\\s|,','', s)\n",
    "            value = float(s)\n",
    "            return value\n",
    "                \n",
    "        else:\n",
    "            return np.nan\n",
    "            \n",
    "    # Clean the box office column in the wiki_movies_df DataFrame.\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.I)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "        \n",
    "    # Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    bedget = budget.str.replace(r'\\$.*[---](?![a-z])', '$', regex=True)\n",
    "\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.I)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    # Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    release_date = wiki_movies_df['Release date'].dropna()\n",
    "    release_date = release_date.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]?\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[0123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    \n",
    "    release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.I)\n",
    "        \n",
    "    # Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    running_time = wiki_movies_df['Running time'].dropna()\n",
    "    running_time = running_time.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    \n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # 2. Clean the Kaggle metadata.\n",
    "    # Keep rows where 'adult' column is False and drop 'adult' column.\n",
    "    kaggle_metadata = kaggle_metadata_df[kaggle_metadata_df['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    \n",
    "    # Create Boolean column and assign it back to 'video'\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    \n",
    "    # Convert 'release_date'\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "    \n",
    "    # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "\n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    movies_df.drop(columns=['title_wiki','Release date','Productioncompany ','Language'], inplace=True)\n",
    "        \n",
    "    # 5. Add in the function to fill in the missing Kaggle data.\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "        \n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "    \n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    for col in movies_df.columns:\n",
    "        lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "        if len(movies_df[col].apply(lists_to_tuples).value_counts(dropna=False)) < 5:\n",
    "            movies_df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # 8. Rename the columns in the movies DataFrame.\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)\n",
    "\n",
    "    # 9. Transform and merge the ratings DataFrame.\n",
    "    # Take the count for each movieId-rating group and rename 'userId' column to 'count'\n",
    "    rating_counts = ratings_df.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1)\n",
    "    \n",
    "    # Pivot the data so that 'movieId' is index, columns are rating values, rows are the counts.\n",
    "    rating_counts =rating_counts.pivot(index='movieId',columns='rating', values='count')\n",
    "    \n",
    "    # Rename columns.\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    \n",
    "    # Merge rating counts, keeping everything in 'movies_df'\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    \n",
    "    # Fill missing values.\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    \n",
    "    # Create connection to PostgreSQL database\n",
    "    db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    # Add 'movies_df' to SQL database.\n",
    "    movies_df.to_sql(name='movies', con=engine)\n",
    "    \n",
    "    # Add 'ratings.csv' to SQL database.\n",
    "    rows_imported = 0\n",
    "    # Get start_time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for data in pd.read_csv(ratings_file, chunksize=1000000):\n",
    "        \n",
    "        # Print out the range of rows that are being imported\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "        \n",
    "        # Increment the number of rows imported\n",
    "        rows_imported += len(data)\n",
    "        \n",
    "        # Print rows have finished importing.\n",
    "        print(f'Complete. {time.time() - start_time} total seconds elapsed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create the path to your file directory and variables for the three files.\n",
    "file_dir = 'C:\\\\Users\\\\happi\\\\Movies-ETL\\\\'\n",
    "\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}wikipedia-movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\happi\\anaconda3\\envs\\PythonData\\lib\\site-packages\\ipykernel_launcher.py:87: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Complete. 49.089471101760864 total seconds elapsed.\n",
      "importing rows 1000000 to 2000000...Complete. 96.73899364471436 total seconds elapsed.\n",
      "importing rows 2000000 to 3000000...Complete. 139.85245537757874 total seconds elapsed.\n",
      "importing rows 3000000 to 4000000...Complete. 182.64520645141602 total seconds elapsed.\n",
      "importing rows 4000000 to 5000000...Complete. 225.2983250617981 total seconds elapsed.\n",
      "importing rows 5000000 to 6000000...Complete. 267.1156163215637 total seconds elapsed.\n",
      "importing rows 6000000 to 7000000...Complete. 306.8283383846283 total seconds elapsed.\n",
      "importing rows 7000000 to 8000000...Complete. 344.2215647697449 total seconds elapsed.\n",
      "importing rows 8000000 to 9000000...Complete. 384.2225000858307 total seconds elapsed.\n",
      "importing rows 9000000 to 10000000...Complete. 421.14152121543884 total seconds elapsed.\n",
      "importing rows 10000000 to 11000000...Complete. 458.3143036365509 total seconds elapsed.\n",
      "importing rows 11000000 to 12000000...Complete. 493.2654330730438 total seconds elapsed.\n",
      "importing rows 12000000 to 13000000...Complete. 528.8277249336243 total seconds elapsed.\n",
      "importing rows 13000000 to 14000000...Complete. 564.3021404743195 total seconds elapsed.\n",
      "importing rows 14000000 to 15000000...Complete. 599.214848279953 total seconds elapsed.\n",
      "importing rows 15000000 to 16000000...Complete. 632.1889839172363 total seconds elapsed.\n",
      "importing rows 16000000 to 17000000...Complete. 658.5652070045471 total seconds elapsed.\n",
      "importing rows 17000000 to 18000000...Complete. 686.389488697052 total seconds elapsed.\n",
      "importing rows 18000000 to 19000000...Complete. 715.1776938438416 total seconds elapsed.\n",
      "importing rows 19000000 to 20000000...Complete. 742.2477192878723 total seconds elapsed.\n",
      "importing rows 20000000 to 21000000...Complete. 770.2426254749298 total seconds elapsed.\n",
      "importing rows 21000000 to 22000000...Complete. 796.8040919303894 total seconds elapsed.\n",
      "importing rows 22000000 to 23000000...Complete. 823.812965631485 total seconds elapsed.\n",
      "importing rows 23000000 to 24000000...Complete. 851.2657260894775 total seconds elapsed.\n",
      "importing rows 24000000 to 25000000...Complete. 878.6632294654846 total seconds elapsed.\n",
      "importing rows 25000000 to 26000000...Complete. 907.6864471435547 total seconds elapsed.\n",
      "importing rows 26000000 to 26024289...Complete. 908.4362094402313 total seconds elapsed.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15508\\177707460.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 11. Set the three variables equal to the function created in D1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwiki_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkaggle_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratings_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_transform_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# 11. Set the three variables equal to the function created in D1.\n",
    "wiki_file, kaggle_file, ratings_file = extract_transform_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Set the DataFrames from the return statement equal to the file names in Step 11. \n",
    "wiki_movies_df = wiki_file\n",
    "movies_with_ratings_df = kaggle_file\n",
    "movies_df = ratings_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\happi\\\\Movies-ETL\\\\wikipedia-movies.json'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13. Check the wiki_movies_df DataFrame. \n",
    "wiki_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\happi\\\\Movies-ETL\\\\movies_metadata.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14. Check the movies_with_ratings_df DataFrame.\n",
    "movies_with_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\happi\\\\Movies-ETL\\\\ratings.csv'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15. Check the movies_df DataFrame. \n",
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
